{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning Transformer Models (OpenAI, Grok and Gemini) for Sentiment Analysis on Twitter Airline Data","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Imports and Initial Setup","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    Trainer, \n    TrainingArguments,\n)\nfrom datasets import Dataset\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport os\nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings(\"ignore\")\n\n# Set PyTorch memory optimization\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Force clear GPU memory at the start\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T01:38:19.267303Z","iopub.execute_input":"2025-04-05T01:38:19.267626Z","iopub.status.idle":"2025-04-05T01:38:19.272582Z","shell.execute_reply.started":"2025-04-05T01:38:19.267601Z","shell.execute_reply":"2025-04-05T01:38:19.271785Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Part 2: Dataset Download and Preprocessing","metadata":{}},{"cell_type":"code","source":"# Download dataset from Kaggle\nos.system(\"kaggle datasets download -d crowdflower/twitter-airline-sentiment\")\nos.system(\"unzip -o twitter-airline-sentiment.zip -d ./airline_data\")\n\n# Load and preprocess dataset\ndf = pd.read_csv(\"./airline_data/Tweets.csv\")\n\n# Keep only positive and negative sentiments\ndf = df[df[\"airline_sentiment\"].isin([\"positive\", \"negative\"])]\n\n# Map labels to binary values\ndf[\"label\"] = df[\"airline_sentiment\"].map({\"positive\": 1, \"negative\": 0})\n\n# Keep only required columns\ndf = df[[\"text\", \"label\"]].dropna()\n\n# Sample a subset (1000 rows)\ndf = df.sample(n=1000, random_state=42)\n\n# Convert to Hugging Face dataset\ndataset = Dataset.from_pandas(df)\n\n# Split into train and eval\ntrain_size = int(0.8 * len(dataset))\ntrain_dataset = dataset.select(range(train_size))\neval_dataset = dataset.select(range(train_size, len(dataset)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T01:38:24.474563Z","iopub.execute_input":"2025-04-05T01:38:24.474916Z","iopub.status.idle":"2025-04-05T01:38:25.937401Z","shell.execute_reply.started":"2025-04-05T01:38:24.474890Z","shell.execute_reply":"2025-04-05T01:38:25.936411Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Part 3: Model Selection & Tokenization Setup","metadata":{}},{"cell_type":"code","source":"# Define models to fine-tune\nmodels = {\n    \"Gemini (FLAN-T5)\": \"google/flan-t5-small\",\n    \"OpenAI (Stand-in)\": \"distilgpt2\",\n    \"Grok (Stand-in)\": \"EleutherAI/gpt-neo-125m\",\n}\n\n# Tokenization function\ndef tokenize_function(examples, tokenizer):\n    return tokenizer(\n        examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128\n    )\n\n# Evaluation metrics\ndef compute_metrics(pred):\n    logits = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions\n    labels = pred.label_ids\n    preds = np.argmax(logits, axis=1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T01:38:31.450729Z","iopub.execute_input":"2025-04-05T01:38:31.451026Z","iopub.status.idle":"2025-04-05T01:38:31.456316Z","shell.execute_reply.started":"2025-04-05T01:38:31.451003Z","shell.execute_reply":"2025-04-05T01:38:31.455597Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Part 4: Fine-Tuning Loop for Each Model","metadata":{}},{"cell_type":"code","source":"# Fine-tune each model\nfor model_name, model_path in models.items():\n    print(f\"\\nFine-tuning {model_name} ({model_path})...\")\n\n    if torch.cuda.is_available():\n        print(f\"GPU Memory - Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GiB, \"\n              f\"Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GiB\")\n\n    try:\n        # Load model and tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        model = AutoModelForSequenceClassification.from_pretrained(\n            model_path,\n            num_labels=2,\n            pad_token_id=tokenizer.pad_token_id,\n            trust_remote_code=True\n        )\n\n        if torch.cuda.is_available():\n            model = model.cuda()\n\n        # Tokenize the datasets\n        tokenized_train = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n        tokenized_eval = eval_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n        tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n        tokenized_eval.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n        # Training settings\n        training_args = TrainingArguments(\n            output_dir=f\"./results/{model_name.replace(' ', '_')}\",\n            num_train_epochs=2,\n            per_device_train_batch_size=2,\n            per_device_eval_batch_size=2,\n            gradient_accumulation_steps=4,\n            warmup_steps=20,\n            weight_decay=0.01,\n            logging_dir=\"./logs\",\n            logging_steps=10,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            learning_rate=2e-5,\n            fp16=True,\n            report_to=\"none\"\n        )\n\n        # Trainer\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_eval,\n            compute_metrics=compute_metrics,\n        )\n\n        # Training\n        trainer.train()\n\n        # Evaluation\n        eval_results = trainer.evaluate()\n        print(f\"Evaluation results for {model_name}: {eval_results}\")\n\n    except Exception as e:\n        print(f\"Error occurred while fine-tuning {model_name}: {e}\")\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    finally:\n        # Clean up memory\n        if 'model' in locals():\n            del model\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            print(f\"Memory cleared - Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GiB, \"\n                  f\"Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GiB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T01:41:58.471075Z","iopub.execute_input":"2025-04-05T01:41:58.471409Z","iopub.status.idle":"2025-04-05T01:46:16.456786Z","shell.execute_reply.started":"2025-04-05T01:41:58.471369Z","shell.execute_reply":"2025-04-05T01:46:16.455956Z"}},"outputs":[{"name":"stdout","text":"\nFine-tuning Gemini (FLAN-T5) (google/flan-t5-small)...\nGPU Memory - Allocated: 0.00 GiB, Reserved: 0.00 GiB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eadaf6ff30bd404994296a3892d533eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eac8b4421621458ab234a1cd01e91e4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7341701ab054732adbcbe87d4e92e32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8598f33c98104f1a9aff4fe9d006448e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6275c24690b8416f959e04a1ad796b99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca96bccd22a435885eab261c8321c0d"}},"metadata":{}},{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"575e0f2eebae43b4b64b34ac5e35661b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5213387e89e9412aa14d877d628cc4ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 01:17, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.741600</td>\n      <td>0.656959</td>\n      <td>0.720000</td>\n      <td>0.243243</td>\n      <td>0.225000</td>\n      <td>0.264706</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.569400</td>\n      <td>0.497894</td>\n      <td>0.830000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight', 'transformer.decoder.embed_tokens.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for Gemini (FLAN-T5): {'eval_loss': 0.4978941082954407, 'eval_accuracy': 0.83, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 3.5643, 'eval_samples_per_second': 56.113, 'eval_steps_per_second': 14.028, 'epoch': 2.0}\nMemory cleared - Allocated: 0.75 GiB, Reserved: 0.80 GiB\n\nFine-tuning OpenAI (Stand-in) (distilgpt2)...\nGPU Memory - Allocated: 0.75 GiB, Reserved: 0.80 GiB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fdcc90eca43471394979fe0a0e48aa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a5c31c98f0d440f9c1cb87b37ea8310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5690b017be6c4dc6be6a17d71484b622"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4548f3940f694d2ebda41f2552f090e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a3cddf08aba4aafb76316ce3781b48f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f289d7229047e89ad6cb7746d5d62d"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bea2fe831604d288a6298ef19814828"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfa4b755889243ce95e007e316eccb1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 00:50, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.417200</td>\n      <td>0.331882</td>\n      <td>0.870000</td>\n      <td>0.409091</td>\n      <td>0.900000</td>\n      <td>0.264706</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.274000</td>\n      <td>0.257175</td>\n      <td>0.895000</td>\n      <td>0.655738</td>\n      <td>0.740741</td>\n      <td>0.588235</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for OpenAI (Stand-in): {'eval_loss': 0.25717493891716003, 'eval_accuracy': 0.895, 'eval_f1': 0.6557377049180328, 'eval_precision': 0.7407407407407407, 'eval_recall': 0.5882352941176471, 'eval_runtime': 2.2377, 'eval_samples_per_second': 89.377, 'eval_steps_per_second': 22.344, 'epoch': 2.0}\nMemory cleared - Allocated: 1.01 GiB, Reserved: 1.06 GiB\n\nFine-tuning Grok (Stand-in) (EleutherAI/gpt-neo-125m)...\nGPU Memory - Allocated: 1.01 GiB, Reserved: 1.06 GiB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12084036c99440c8ea1ea1c4dea171e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cce40ca5838428393f41ae0460c86e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dc1921a2b9f419893449ff89c0afc7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a010cbae9849bd82b2e2370e442527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47360ce36f164ddbae130d3e27982e67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f07bab991cef45b4a7d556fe88c6d941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677f9eb3eeb14f799dfe1d285b0a35e4"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125m and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e15eebf46f458f9841d94fbb683ee0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64db1f73dd61424a88beca9f356b3052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 01:27, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.395600</td>\n      <td>0.347843</td>\n      <td>0.870000</td>\n      <td>0.480000</td>\n      <td>0.750000</td>\n      <td>0.352941</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.236600</td>\n      <td>0.287624</td>\n      <td>0.880000</td>\n      <td>0.571429</td>\n      <td>0.727273</td>\n      <td>0.470588</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for Grok (Stand-in): {'eval_loss': 0.2876235246658325, 'eval_accuracy': 0.88, 'eval_f1': 0.5714285714285714, 'eval_precision': 0.7272727272727273, 'eval_recall': 0.47058823529411764, 'eval_runtime': 3.886, 'eval_samples_per_second': 51.466, 'eval_steps_per_second': 12.867, 'epoch': 2.0}\nMemory cleared - Allocated: 1.57 GiB, Reserved: 1.70 GiB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Part 5: Save and Run Inference","metadata":{}},{"cell_type":"code","source":"for model_name, model_path in models.items():\n print(f\"\\nFine-tuning {model_name} ({model_path})...\")\n\n if torch.cuda.is_available():\n  print(f\"GPU Memory - Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GiB, \"\n        f\"Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GiB\")\n\n try:\n  # Load model and tokenizer\n  tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n  if tokenizer.pad_token is None:\n   tokenizer.pad_token = tokenizer.eos_token\n  model = AutoModelForSequenceClassification.from_pretrained(\n   model_path,\n   num_labels=2,\n   pad_token_id=tokenizer.pad_token_id,\n   trust_remote_code=True\n  )\n\n  if torch.cuda.is_available():\n   model = model.cuda()\n\n  # Tokenize the datasets\n  tokenized_train = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n  tokenized_eval = eval_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n  tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n  tokenized_eval.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n  # Training settings\n  training_args = TrainingArguments(\n   output_dir=\"./results\",\n   num_train_epochs=2,\n   per_device_train_batch_size=2,\n   per_device_eval_batch_size=2,\n   gradient_accumulation_steps=4,\n   warmup_steps=20,\n   weight_decay=0.01,\n   logging_dir=\"./logs\",\n   logging_steps=10,\n   evaluation_strategy=\"epoch\",\n   save_strategy=\"epoch\",\n   load_best_model_at_end=True,\n   learning_rate=2e-5,\n   fp16=True,\n   report_to=\"none\"\n  )\n\n  # Trainer\n  trainer = Trainer(\n   model=model,\n   args=training_args,\n   train_dataset=tokenized_train,\n   eval_dataset=tokenized_eval,\n   compute_metrics=compute_metrics,\n  )\n\n  # Train\n  trainer.train()\n\n  # Evaluate\n  eval_results = trainer.evaluate()\n  print(f\"Evaluation results for {model_name}: {eval_results}\")\n\n  # Save fine-tuned model and tokenizer\n  save_dir = f\"./fine_tuned_{model_name.lower().replace(' ', '_')}\"\n  model.save_pretrained(save_dir)\n  tokenizer.save_pretrained(save_dir)\n\n  # Inference function\n  def predict(text, model, tokenizer):\n   inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n   inputs = {k: v.to(model.device) for k, v in inputs.items()}\n   with torch.no_grad():\n    outputs = model(**inputs)\n   logits = outputs.logits\n   prediction = torch.argmax(logits, dim=-1).item()\n   return \"Positive\" if prediction == 1 else \"Negative\"\n\n  # Sample prediction\n  sample_text = \"Great flight with excellent service!\"\n  print(f\"Prediction for '{sample_text}' with {model_name}: {predict(sample_text, model, tokenizer)}\")\n\n except torch.cuda.OutOfMemoryError:\n  print(f\"Out of memory error for {model_name}. Skipping this model.\")\n  if torch.cuda.is_available():\n   torch.cuda.empty_cache()\n  continue\n\n except ValueError as e:\n  print(f\"ValueError for {model_name}: {e}. Skipping this model.\")\n  if torch.cuda.is_available():\n   torch.cuda.empty_cache()\n  continue\n\n finally:\n  # Clear GPU memory after each model\n  if 'model' in locals():\n   del model\n  if torch.cuda.is_available():\n   torch.cuda.empty_cache()\n   print(f\"Memory cleared - Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GiB, \"\n         f\"Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GiB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T02:05:56.679678Z","iopub.execute_input":"2025-04-05T02:05:56.679996Z","iopub.status.idle":"2025-04-05T02:09:51.048526Z","shell.execute_reply.started":"2025-04-05T02:05:56.679974Z","shell.execute_reply":"2025-04-05T02:09:51.046245Z"}},"outputs":[{"name":"stdout","text":"\nFine-tuning Gemini (FLAN-T5) (google/flan-t5-small)...\nGPU Memory - Allocated: 1.57 GiB, Reserved: 1.70 GiB\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"315c9b8bdfa24c5d93d6ea28ec0c1efd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e3667bb5fbd41cb9aed0e52aa3c09fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 01:15, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.522800</td>\n      <td>0.435395</td>\n      <td>0.830000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.449300</td>\n      <td>0.404598</td>\n      <td>0.830000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight', 'transformer.decoder.embed_tokens.weight'].\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for Gemini (FLAN-T5): {'eval_loss': 0.40459808707237244, 'eval_accuracy': 0.83, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 3.9721, 'eval_samples_per_second': 50.351, 'eval_steps_per_second': 12.588, 'epoch': 2.0}\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Prediction for 'Great flight with excellent service!' with Gemini (FLAN-T5): Negative\nMemory cleared - Allocated: 0.75 GiB, Reserved: 0.88 GiB\n\nFine-tuning OpenAI (Stand-in) (distilgpt2)...\nGPU Memory - Allocated: 0.75 GiB, Reserved: 0.88 GiB\n","output_type":"stream"},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57bda4325e8349669fdbb040d292ff6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d41389962ab477abd51f9922cff72a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 00:51, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.417200</td>\n      <td>0.331882</td>\n      <td>0.870000</td>\n      <td>0.409091</td>\n      <td>0.900000</td>\n      <td>0.264706</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.274000</td>\n      <td>0.257175</td>\n      <td>0.895000</td>\n      <td>0.655738</td>\n      <td>0.740741</td>\n      <td>0.588235</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for OpenAI (Stand-in): {'eval_loss': 0.25717493891716003, 'eval_accuracy': 0.895, 'eval_f1': 0.6557377049180328, 'eval_precision': 0.7407407407407407, 'eval_recall': 0.5882352941176471, 'eval_runtime': 2.1885, 'eval_samples_per_second': 91.387, 'eval_steps_per_second': 22.847, 'epoch': 2.0}\nPrediction for 'Great flight with excellent service!' with OpenAI (Stand-in): Positive\nMemory cleared - Allocated: 1.01 GiB, Reserved: 1.08 GiB\n\nFine-tuning Grok (Stand-in) (EleutherAI/gpt-neo-125m)...\nGPU Memory - Allocated: 1.01 GiB, Reserved: 1.08 GiB\n","output_type":"stream"},{"name":"stderr","text":"Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125m and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e15a02186d484da874598e2ebb631b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a25001ce02c4a3592c36f47609dbd0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 01:27, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.395600</td>\n      <td>0.347843</td>\n      <td>0.870000</td>\n      <td>0.480000</td>\n      <td>0.750000</td>\n      <td>0.352941</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.236600</td>\n      <td>0.287624</td>\n      <td>0.880000</td>\n      <td>0.571429</td>\n      <td>0.727273</td>\n      <td>0.470588</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results for Grok (Stand-in): {'eval_loss': 0.2876235246658325, 'eval_accuracy': 0.88, 'eval_f1': 0.5714285714285714, 'eval_precision': 0.7272727272727273, 'eval_recall': 0.47058823529411764, 'eval_runtime': 3.8524, 'eval_samples_per_second': 51.916, 'eval_steps_per_second': 12.979, 'epoch': 2.0}\nPrediction for 'Great flight with excellent service!' with Grok (Stand-in): Positive\nMemory cleared - Allocated: 1.58 GiB, Reserved: 1.68 GiB\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Conclusion:\n\nIn this notebook, we fine-tuned multiple transformer models, including FLAN-T5, GPT-2, and GPT-Neo, for sentiment analysis on Twitter airline data. We used the Hugging Face library for model loading, tokenization, training, and evaluation. After fine-tuning the models, we assessed their performance using an evaluation strategy based on epochs and saved the fine-tuned models for future inference tasks.","metadata":{}}]}